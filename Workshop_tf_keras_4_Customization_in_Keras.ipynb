{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Workshop tf keras 4. Customization in Keras.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQsYyteusHuE"
      },
      "source": [
        "**Writing Custom layers and Models**<br/>\n",
        "We can make custom layers and models in keras by subclassing, the layer class and the model class.\n",
        "\n",
        "**The Layer Class** <br/>\n",
        "\n",
        "One of the central abstarction in Keras is the Layer class. \n",
        "1. It encapsulates the weights and a computation (i.e. call, the layer's forward pass). \n",
        "2. A layer can have trainable as well as non - trainable weights. \n",
        "3. Layers can be composed recursively.\n",
        "4. Some layers, in particular the BatchNormalization layer and the Dropout layer, have different behaviors during training and inference. For such layers, it is standard practice to expose a training (boolean) argument in the call() method.\n",
        "\n",
        "**The Model Class**<br/>\n",
        "Typically, a layer defines an inner computation block and the model defines the overall architecture, i.e the obeject which we train. Here we have 3 ResNet blocks which subclass (inherit) Layer class and a single ResNetModel which contains the three ResNetBlocks. It is similar to Layer class except that it allows to use model.compile(), model.fit() etc.\n",
        "\n",
        "\n",
        "Refer to https://www.tensorflow.org/guide/keras/custom_layers_and_models#the_model_class for more details"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C55laZ0hJX_J"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oo-WiRDQUyS"
      },
      "source": [
        "class ResNetBlock(tf.keras.layers.Layer): # A ResNet Block comprising of two convolution blocks followed by max_pooling for the first layer.\n",
        "\n",
        "  def __init__(self, out_channels, kernel_size,  padding = \"valid\", max_pool = False):\n",
        "    #out_channels - the list of no. of filters to be used in the convolution operations,\n",
        "    #kernel_size - the size of the kernel to be used\n",
        "    #max_pool - Falsem for the second and third layers and True for teh first layer. Allows MaxPooling in the first layer.\n",
        "    super(ResNetBlock, self).__init__(name = 'Block')\n",
        "    self.conv1 = tf.keras.layers.Conv2D(filters = out_channels[0], kernel_size = kernel_size, padding = padding, activation = \"relu\")\n",
        "    self.conv2 = tf.keras.layers.Conv2D(filters = out_channels[1], kernel_size = kernel_size, padding = padding, activation = \"relu\")\n",
        "    self.max_pool = max_pool\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    x = self.conv1(inputs)\n",
        "    x = self.conv2(x)\n",
        "    if self.max_pool:\n",
        "      x = tf.keras.layers.MaxPooling2D(3)(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Uq2jgPvUxOc"
      },
      "source": [
        "class ResNetModel(tf.keras.Model): # The ResNet Model comprising of the three ResNet Blocks and the remaining operations liek adding skip connections.\n",
        "\n",
        "  def __init__(self):\n",
        "    super(ResNetModel, self).__init__()\n",
        "    self.block1 = ResNetBlock(out_channels = [32, 64], kernel_size = 3, max_pool = True)\n",
        "    self.block2 = ResNetBlock(out_channels = [64, 64], kernel_size = 3, padding = \"same\")\n",
        "    self.block3 = ResNetBlock(out_channels = [64, 64], kernel_size = 3, padding = \"same\")\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.conv = tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, activation = \"relu\")\n",
        "    self.global_average_pooling = tf.keras.layers.GlobalAveragePooling2D()\n",
        "    self.dense1 = tf.keras.layers.Dense(256, activation = \"relu\")\n",
        "    self.dropout = tf.keras.layers.Dropout(0.5)\n",
        "    self.dense2 = tf.keras.layers.Dense(10, activation= \"softmax\")\n",
        "  \n",
        "  def call(self, inputs, training = False):\n",
        "    block1_output = self.block1(inputs)\n",
        "    block2_output = self.block2(block1_output)\n",
        "    block2_output = self.add([block1_output, block2_output])\n",
        "    block3_output = self.block3(block2_output)\n",
        "    block3_output = self.add([block2_output, block3_output])\n",
        "    x = self.conv(block3_output)\n",
        "    x = self.global_average_pooling(x)\n",
        "    x = self.dense1(x)\n",
        "    x = self.dropout(x, training = training)\n",
        "    x = self.dense2(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugHtRxHfYFC-"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbxbRuzdY743"
      },
      "source": [
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10) #convert to one hot, since we will be using categorical cross entropy.\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuqA86j1Zd6v"
      },
      "source": [
        "lr = 1e-3\n",
        "optimizer = tf.keras.optimizers.RMSprop(lr = lr)\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy() #Note the differnce between categorical cross entropy and sparse categorical cross entropy.\n",
        "batch_size = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcByAoX5Y_xA"
      },
      "source": [
        "model = ResNetModel()\n",
        "model.compile(optimizer = optimizer, loss = loss_fn, metrics = [\"acc\"])\n",
        "history = model.fit(x_train[:15000], y_train[:15000], epochs=20, validation_split = 0.3) #Also add saving complete model, model checkpointing only stores the"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1YcPWTXZK02"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "print(history.history.keys())\n",
        "#  \"Accuracy\"\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "# \"Loss\"\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdrJcRoWxMOb"
      },
      "source": [
        "**Writing a training loop from scratch**<br/>\n",
        "It provides very low level control over training and validation. <br/>\n",
        "Typical workflow of a training loop:\n",
        "1. Open a loop to iterate over epochs.\n",
        "2. Over each epoch, open a loop to iterate over batches (steps).\n",
        "3. Open a GradientTape scope and call the model, keep training =  true, in this step and compute the loss.\n",
        "4. Outside the scope retrieve the gradients w.r.t. loss and update the weights.\n",
        "5. You can add metrics, and updating them appropriately in the loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuLPCGIxaIwX",
        "outputId": "c5e55ff9-2765-4e1e-a70b-7f516f63e148",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train[:15000], y_train[:15000], test_size = 0.3)\n",
        "model = ResNetModel() #Get the model.\n",
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10500, 32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Sze4wcmpt5R"
      },
      "source": [
        "#Create tensorflow Datasets.\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(buffer_size = 1024).batch(batch_size)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size)\n",
        "# Prepare the metrics.\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "val_acc_metric = tf.keras.metrics.CategoricalAccuracy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5vbTPPgqajs"
      },
      "source": [
        "import time\n",
        "\n",
        "epochs = 20\n",
        "#iterate over the epochs\n",
        "for epoch in range(epochs): \n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "      #Calling a model inside a GradientTape scope enables you to retrieve the gradients \n",
        "      #of the trainable weights of the layer with respect to a loss value. \n",
        "      #Using an optimizer instance, you can use these gradients to update these variables (which you can retrieve using model.trainable_weights).\n",
        "      #It records the operations run during the forward pass and enables auto-differentiation.\n",
        "        with tf.GradientTape() as tape:\n",
        "            preds = model(x_batch_train, training=True) #Predcition for the mini-batch.\n",
        "            #Compute the loss for this mini-batch.\n",
        "            loss_value = loss_fn(y_batch_train, preds) \n",
        "        #Retieve the gradients from the gradient tape of the trainable variables.\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "\n",
        "        #Update the weights by running one-step of the optimizer.\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "        # Update training metric.\n",
        "        train_acc_metric.update_state(y_batch_train, logits)\n",
        "\n",
        "        # Log every 200 batches.\n",
        "        if step % 200 == 0:\n",
        "            print(\n",
        "                \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                % (step, float(loss_value))\n",
        "            )\n",
        "            print(\"Seen so far: %d samples\" % ((step + 1) * 64))\n",
        "\n",
        "    # Display metrics at the end of each epoch.\n",
        "    train_acc = train_acc_metric.result()\n",
        "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
        "\n",
        "    # Reset training metrics at the end of each epoch\n",
        "    train_acc_metric.reset_states()\n",
        "\n",
        "    # Run a validation loop at the end of each epoch.\n",
        "    for x_batch_val, y_batch_val in val_dataset:\n",
        "        val_logits = model(x_batch_val, training=False)\n",
        "        # Update val metrics\n",
        "        val_acc_metric.update_state(y_batch_val, val_logits)\n",
        "    val_acc = val_acc_metric.result()\n",
        "    val_acc_metric.reset_states()\n",
        "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
        "    print(\"Time taken: %.2fs\" % (time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgYPWis7r_ma"
      },
      "source": [
        "lr = 1e-3\n",
        "optimizer = tf.keras.optimizers.RMSprop(lr = lr)\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy() #Note the differnce between categorical cross entropy and sparse categorical cross entropy.\n",
        "batch_size = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fksoW4d_ri06"
      },
      "source": [
        "#for optimization purpose, refer to https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch for the details\n",
        "#as to why this helps to improve performance.\n",
        "@tf.function\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x, training=True)\n",
        "        loss_value = loss_fn(y, logits)\n",
        "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    train_acc_metric.update_state(y, logits)\n",
        "    return loss_value\n",
        "\n",
        "@tf.function\n",
        "def test_step(x, y):\n",
        "    val_logits = model(x, training=False)\n",
        "    val_acc_metric.update_state(y, val_logits)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jNUXtAaq3lB"
      },
      "source": [
        "import time\n",
        "\n",
        "model = ResNetModel()\n",
        "\n",
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        loss_value = train_step(x_batch_train, y_batch_train) #Run the train step.\n",
        "\n",
        "        # Log every 200 batches.\n",
        "        if step % 200 == 0:\n",
        "            print(\n",
        "                \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                % (step, float(loss_value))\n",
        "            )\n",
        "            print(\"Seen so far: %d samples\" % ((step + 1) * 64))\n",
        "\n",
        "    # Display metrics at the end of each epoch.\n",
        "    train_acc = train_acc_metric.result()\n",
        "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
        "\n",
        "    # Reset training metrics at the end of each epoch\n",
        "    train_acc_metric.reset_states()\n",
        "\n",
        "    # Run a validation loop at the end of each epoch.\n",
        "    for x_batch_val, y_batch_val in val_dataset:\n",
        "        test_step(x_batch_val, y_batch_val) #Run the validation step.\n",
        "\n",
        "    val_acc = val_acc_metric.result()\n",
        "    val_acc_metric.reset_states()\n",
        "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
        "    print(\"Time taken: %.2fs\" % (time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Axx_5wK0ruWx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}